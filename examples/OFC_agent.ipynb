{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optical RL-Gym\n",
    "\n",
    "## Training the Stable Baselines agents using the DeepRLSA environment\n",
    "\n",
    "This file contains examples of how to train agents for the DeepRMSA environment.\n",
    "\n",
    "The agents used in this file come from the [Stable baselines](https://github.com/hill-a/stable-baselines) framework.\n",
    "\n",
    "This notebook is based upon the one available [here](https://github.com/Stable-Baselines-Team/rl-colab-notebooks/blob/master/monitor_training.ipynb).\n",
    "\n",
    "Before running this notebook, make sure to install Stable Baselines and the Optical RL-Gym in your Python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# silencing tensorflow warnings\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "tf.__version__ # printint out tensorflow version used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stable Baseline imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/acid/Documents/Projects/explainable/.venv_explainable/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.4.1a0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stable_baselines3\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy as MLP_PPO\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn.policies import MlpPolicy as MLP_DQN\n",
    "from sb3_contrib import TRPO\n",
    "from sb3_contrib.trpo import MlpPolicy as MLP_TRPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common import results_plotter\n",
    "\n",
    "stable_baselines3.__version__ # printing out stable_baselines version used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment imports\n",
    "\n",
    "In this particular example, there is no need to import anything specific to the Optical RL-Gym. Only by importing the Open AI Gym below, you already get all the functionality needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a callback function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback from https://stable-baselines.readthedocs.io/en/master/guide/examples.html#using-callback-monitoring-training\n",
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = log_dir\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        # if self.save_path is not None:\n",
    "        #     os.makedirs(self.save_path, exist_ok=True)\n",
    "        return\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "            if len(x) > 0:\n",
    "                 # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(\"Num timesteps: {} - \".format(self.num_timesteps), end=\"\")\n",
    "                    print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
    "                  # New best model, you could save the agent here\n",
    "                if mean_reward >= self.best_mean_reward and self.num_timesteps>2000:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(\"Saving new best model to {}\".format(self.save_path))\n",
    "                        self.model.save(self.save_path+'best_model.zip')\n",
    "                if self.verbose > 0:\n",
    "                    clear_output(wait=True)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment\n",
    "\n",
    "The parameters are set as in the [DeepRMSA](https://doi.org/10.1109/JLT.2019.2923615) work and its [available reporitory](https://github.com/xiaoliangchenUCD/DeepRMSA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1001400 - Best mean reward: 44.44 - Last mean reward per episode: 41.92\n"
     ]
    }
   ],
   "source": [
    "from explainable.utils import linear_schedule\n",
    "\n",
    "alg_name = 'PPO'\n",
    "top_name = 'nsfnet'\n",
    "k_path = 3\n",
    "\n",
    "\n",
    "\n",
    "topology_dir = '/topologies/demo/' +  top_name +f'_{k_path}.h5'\n",
    "with open(f'..{topology_dir}', 'rb') as f:\n",
    "    topology = pickle.load(f)\n",
    "\n",
    "node_request_probabilities = np.array([])\n",
    "if top_name == 'arpanet':\n",
    "    # ---------------------- ARPANET ----------------------\n",
    "    node_request_probabilities = np.array(\n",
    "        [0.10131117, 0.12078696, 0.06144304, 0.00394418, 0.06218475,\n",
    "        0.04044608, 0.09256297, 0.02113283, 0.02084576, 0.07330581,\n",
    "        0.04822402, 0.01407012, 0.0251201 , 0.04523283, 0.12847282,\n",
    "        0.01805554, 0.01488939, 0.03013041, 0.06110418, 0.01673704]\n",
    "    )\n",
    "elif top_name == 'eon':\n",
    "    # ---------------------- EON ----------------------\n",
    "    node_request_probabilities = np.array(\n",
    "        [0.13956028, 0.02775406, 0.1583229 , 0.01276534, 0.06687379,\n",
    "        0.02519223, 0.02306825, 0.01166695, 0.0594671 , 0.00071904,\n",
    "        0.06957169, 0.13642354, 0.03778149, 0.05543918, 0.07873654,\n",
    "        0.02076745, 0.00419003, 0.02320005, 0.01052078, 0.03797931]\n",
    "    )\n",
    "elif top_name == 'eurocore':\n",
    "    # ---------------------- EUROCORE ----------------------\n",
    "    node_request_probabilities = np.array(\n",
    "        [0.01711661, 0.05418066, 0.11466408, 0.37467221, 0.01244822,\n",
    "        0.00672383, 0.00170215, 0.14903192, 0.20510173, 0.02759766,\n",
    "        0.03676094]\n",
    "    )\n",
    "elif top_name == 'italiana':\n",
    "    # ---------------------- ITALIANA ----------------------\n",
    "    node_request_probabilities = np.array(\n",
    "        [0.06646663, 0.28975685, 0.04804817, 0.12453275, 0.09512295,\n",
    "        0.05196806, 0.02895454, 0.0071567 , 0.02332887, 0.05678903,\n",
    "        0.0026715 , 0.00254033, 0.0123507 , 0.00230415, 0.02801925,\n",
    "        0.00800734, 0.01208697, 0.02598813, 0.06745542, 0.02508004,\n",
    "        0.0213716]\n",
    "    )\n",
    "elif top_name == 'nsfnet':\n",
    "    # ---------------------- NSFNET ----------------------\n",
    "    node_request_probabilities = np.array([0.01801802, 0.04004004, 0.05305305, 0.01901902, 0.04504505,\n",
    "        0.02402402, 0.06706707, 0.08908909, 0.13813814, 0.12212212,\n",
    "        0.07607608, 0.12012012, 0.01901902, 0.16916917])\n",
    "elif top_name == 'uknet':\n",
    "    # ---------------------- UKNET ----------------------\n",
    "    node_request_probabilities = np.array(\n",
    "        [0.06646663, 0.28975685, 0.04804817, 0.12453275, 0.09512295,\n",
    "        0.05196806, 0.02895454, 0.0071567 , 0.02332887, 0.05678903,\n",
    "        0.0026715 , 0.00254033, 0.0123507 , 0.00230415, 0.02801925,\n",
    "        0.00800734, 0.01208697, 0.02598813, 0.06745542, 0.02508004,\n",
    "        0.0213716]\n",
    "    )\n",
    "else:\n",
    "    # ---------------------- USNET ----------------------\n",
    "    node_request_probabilities = np.array(\n",
    "        [1.23807304e-02, 2.92335629e-02, 2.62436887e-06, 8.26014201e-03,\n",
    "        3.64143708e-03, 2.22290607e-03, 4.72909952e-03, 9.72772742e-03,\n",
    "        1.15971163e-02, 1.77577532e-02, 1.24663926e-02, 2.65205341e-02,\n",
    "        5.24785875e-03, 4.82902294e-02, 6.37146993e-04, 2.54697119e-02,\n",
    "        1.23918630e-02, 1.87683811e-02, 3.47080980e-03, 5.06542659e-03,\n",
    "        3.70125617e-02, 7.91621028e-02, 8.62783971e-03, 2.70442037e-02,\n",
    "        4.79671702e-02, 5.16253403e-02, 2.03925432e-03, 9.14041312e-04,\n",
    "        4.27046339e-03, 4.82949487e-02, 2.37528831e-03, 1.25420925e-02,\n",
    "        7.26742589e-02, 1.74783004e-02, 2.70110059e-02, 8.69783866e-03,\n",
    "        2.66141267e-02, 4.12887779e-02, 4.23491085e-04, 3.18204224e-02,\n",
    "        1.03186416e-01, 3.16394394e-02, 7.55135878e-03, 3.57289387e-02,\n",
    "        2.49978391e-03, 1.36290810e-02]\n",
    "    )\n",
    "\n",
    "env_args = dict(topology=topology, seed=10, \n",
    "                allow_rejection=False, \n",
    "                k_paths=k_path,\n",
    "                j=1,\n",
    "                mean_service_holding_time=7.5, \n",
    "                mean_service_inter_arrival_time=0.1,\n",
    "                episode_length=50, node_request_probabilities=node_request_probabilities,\n",
    "                only_spectrum_obs = False)\n",
    "\n",
    "### Creating the monitors and agent\n",
    "# Create log dir\n",
    "log_dir = \"./tmp/\" + top_name + f\"_{k_path}/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=100, log_dir=log_dir)\n",
    "env = gym.make('DeepRMSA-v0', **env_args)\n",
    "\n",
    "# logs will be saved in log_dir/training.monitor.csv\n",
    "# in this case, on top of the usual monitored things, we also monitor service and bit rate blocking rates\n",
    "env = Monitor(env, log_dir, info_keywords=('episode_service_blocking_rate','episode_bit_rate_blocking_rate'))\n",
    "# for more information about the monitor, check https://stable-baselines.readthedocs.io/en/master/_modules/stable_baselines/bench/monitor.html#Monitor\n",
    "\n",
    "# here goes the arguments of the policy network to be used\n",
    "policy_args = dict(net_arch=5*[128]) # we use the elu activation function\n",
    "tensorboard_log = \"./tb/\" + top_name + f\"_{k_path}/\"\n",
    "\n",
    "if alg_name == 'PPO':\n",
    "    agent = PPO(MLP_PPO, env, verbose=0, tensorboard_log=tensorboard_log, policy_kwargs=policy_args, gamma=.95, learning_rate=linear_schedule(5*10e-5, 5*10e-6), seed=10)\n",
    "elif alg_name == 'DQN':\n",
    "    agent = DQN(MLP_DQN, env, verbose=0, tensorboard_log=tensorboard_log, policy_kwargs=policy_args, gamma=.95, learning_rate=linear_schedule(10e-4),seed=10)\n",
    "else: # TRPO\n",
    "    agent = TRPO(MLP_TRPO, env, verbose=0, tensorboard_log=tensorboard_log, policy_kwargs=policy_args, gamma=.95, learning_rate=linear_schedule(10e-4),seed=10)\n",
    "\n",
    "### Training the agent:\n",
    "a = agent.learn(total_timesteps=1000000, callback=callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv_explainable': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "85738635c78e4a7cd1f03905bfc0e59fb808e736b488f02ee4e2418032da2be0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
